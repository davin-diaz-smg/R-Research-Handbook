[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Handbook for Research",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Key Drivers models",
    "section": "",
    "text": "“Multiple regression is perhaps the most frequently used statistical tool for the analysis of data in the organizational sciences. The information provided by such analyses is particularly useful for addressing issues related to prediction such as identifying a set of predictors that will maximize the amount of variance explained in the criterion. However, most researchers and practitioners are simultaneously interested in multiple regression for theory testing or explanation purposes. Here, the question of interest becomes understanding the extent to which each variable drives the prediction. Essentially, one wishes to understand the contribution each predictor makes towards explaining variance in the criterion. Past research has documented how indices commonly produced by multiple regression analyses fail to appropriately partition variance to the various predictors when they are correlated (Darlington 1968). In response, two alternative approaches, dominance analysis (Budescu 1993) and relative weight analysis (Fabbris 1980; Johnson 2000), have been developed that allow for more accurate variance partitioning among correlated predictors.”\nFrom “Relative Importance Analysis: A Useful Supplement to Regression Analysis” – found here\nRelative weights analysis uses the correlations between the dependent and independent variables and determines what percentage of the explained variance can be attributed back to each independent variable. The benefit to relative weights is that these percentages are additive, meaning that you can sum up the percentages to create groups with similar themes. For instance, questions about the helpfulness, friendliness, and knowledge of associates can be grouped to create an associates factor.\n\n\n\nRelative weights are specifically developed for use with correlated predictors\nConsiders the predictive value of the variable both in isolation and in the presence of other variables\nThe non-scaled weights sum to the R-squared and thus can be used as estimates of relative effect sizes\nRelative weights are easily rescaled into proportions or percentages which is easily communicated to clients\n\n\n\n\nKey Driver Analyses are the key deliverable and the bread-and-butter of the Research Department. With key driver analysis, we are concerned with evaluating the relative importance of different aspects of the customer’s product and service experience in predicting their Overall Satisfaction.\nThe Relative Weights analysis works very well with the types of variables we utilize in our analyses at SMG, as they are usually very highly correlated. When we run a linear regression, it typically results in one variable taking the lion’s share of variance as the top driver, while the rest fall in at a much smaller variance due to the nature of linear regressions. Using Relative Weights allows us to assign that shared variance back to the original variables, which results in a much more balanced predictive model. In addition, it’s easier for clients to understand as the rescaled importance weights sum to 100%.\nOur Key Driver models are filtered down to only 4’s & 5’s (or satisfied and highly satisfied) for Overall Satisfaction. We do this to hone in on what is most important to driving loyalty. When we look at our loyalty curve crosstabs (% Highly Likely to Return or Recommend broken out by Overall Satisfaction rating), we see that moving a 4 (Satisfied) on OSAT to a 5 (Highly Satisfied) on OSAT results in a two to three times greater % Highly Likely to Return or Recommend. In addition, normally about two thirds of guests fall into these two buckets, so it’s very representative of the survey guest base. If a client is interested in a full-scale approach, consider running Rewards & Penalties, covered later in this section as well as in section 8 of the training manual “SMart Gen II”, Analyses: Key Driver (Relative Weights)."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Quick R tips",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "intro.html#running-the-key-driver-analysis",
    "href": "intro.html#running-the-key-driver-analysis",
    "title": "2  Key Drivers models",
    "section": "2.2 Running the Key Driver Analysis",
    "text": "2.2 Running the Key Driver Analysis\n\n2.2.1 Materials & Applications to run Key Drivers\nThe Key Driver analysis has been automated to run in SMart, see section 8, Analyses: Key Driver (Relative Weights) Analysis for further information. However, we still occasionally run Key Drivers manually in R via the SMG package using the key_driver() function.\n\n\n2.2.2 Pull the data\nFor this example we are going to use data from United Dairy Farms. To pull the data directly from SQL we use the following syntax:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.1     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(odbc)\n\nWarning: package 'odbc' was built under R version 4.1.3\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.1.3\n\n\ncorrplot 0.92 loaded\n\nlibrary(papeR)\n\nWarning: package 'papeR' was built under R version 4.1.3\n\n\nLoading required package: car\n\n\nWarning: package 'car' was built under R version 4.1.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.1.3\n\n\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: xtable\n\n\nWarning: package 'xtable' was built under R version 4.1.3\n\n\nRegistered S3 method overwritten by 'papeR':\n  method    from\n  Anova.lme car \n\nAttaching package: 'papeR'\n\nThe following objects are masked from 'package:dplyr':\n\n    summarise, summarize\n\nThe following object is masked from 'package:utils':\n\n    toLatex\n\nlibrary(gt)\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.1.3\n\nconnection <- dbConnect(odbc(),\n                        Driver = \"SQL Server\",\n                        Server = \"SRV_INT_TRANS\\\\SMGTRANS\",\n                        Trusted_Connection = \"True\")\n\nudf_csi_kd <- dbGetQuery(connection,\n\"SELECT \nUDF_CSI_Calls.[UID], \nUDF_CSI_Calls.[R000005], \nUDF_CSI_Calls.[R000017], \nUDF_CSI_Calls.[R000018], \nUDF_CSI_Calls.[R000020], \nUDF_CSI_Calls.[R000022], \nUDF_CSI_Calls.[R000029]\nFROM \nedify.dbo.UDF_CSI_Calls WITH (NOLOCK)  \nINNER JOIN structures.dbo.UDF_CSI_Store_Info WITH (NOLOCK) ON UDF_CSI_Calls.[StoreId] = UDF_CSI_Store_Info.[StoreId] \nWHERE \nUDF_CSI_Store_Info.[IN_CSI] = -1 AND UDF_CSI_Store_Info.[TEST_STORE] = 0\nAND UDF_CSI_Calls.[Date_Time] BETWEEN '2022-01-01 12:00:00 AM' AND '2022-12-31 11:59:59 PM'\nAND UDF_CSI_Calls.[TERM] IN ('OK', 'WebOK')\")\n\nIn this case we only pull the variables that we need for the analysis.\n\n\n2.2.3 Prepare data\nIn order to get the data into the right format, it is important to perform the following steps:\n\nSelect only the variables that we are going to use including OSAT and the explanatory variables (make sure OSAT is the first variable in the dataset)\nKeep only complete cases\nFilter OSAT so it only includes 4s and 5s\n\n-Convert 4s into 0s and 5s into 1s\nAll those steps can be performed in a single run of code using the tidyverse like this:\n\nudf_csi_kd <- udf_csi_kd%>%\n  select(-UID)%>%\n  filter_all(all_vars(.%in% 1:5))%>%\n  filter(R000005 %in% 4:5)%>%\n  mutate(R000005 = case_when(R000005 == 4 ~ 0, R000005 == 5 ~ 1))\n\n\n\n2.2.4 Get correlations\nThe easiest way to run simple correlations between the data is using the following code:\n\nudf_csi_kd%>%\n  cor()\n\n          R000005   R000017   R000018   R000020   R000022   R000029\nR000005 1.0000000 0.5833673 0.5206141 0.5725330 0.5933566 0.5278057\nR000017 0.5833673 1.0000000 0.5553600 0.6268914 0.6799234 0.5358979\nR000018 0.5206141 0.5553600 1.0000000 0.5502581 0.5876474 0.5350537\nR000020 0.5725330 0.6268914 0.5502581 1.0000000 0.7743361 0.4975037\nR000022 0.5933566 0.6799234 0.5876474 0.7743361 1.0000000 0.5410453\nR000029 0.5278057 0.5358979 0.5350537 0.4975037 0.5410453 1.0000000\n\n\n\n\n2.2.5 Get descriptives\n\n\n2.2.6 Run key drivers function\nThe key drivers function is built so it only needs two main inputs, the dataset itself and the name of the project. Once all the previous steps have been performed, you can easily run the model with these lines of code\n\nudf_kd_model <- udf_csi_kd%>%\n  key_drivers(\"UDF_CSI\")\n\nWarning in labels(data)[j] <- var_label: number of items to replace is not a\nmultiple of replacement length\n\nWarning in labels(data)[j] <- var_label: number of items to replace is not a\nmultiple of replacement length\n\nWarning in labels(data)[j] <- var_label: number of items to replace is not a\nmultiple of replacement length\n\nWarning in labels(data)[j] <- var_label: number of items to replace is not a\nmultiple of replacement length\n\nWarning in labels(data)[j] <- var_label: number of items to replace is not a\nmultiple of replacement length\n\nWarning in labels(data)[j] <- var_label: number of items to replace is not a\nmultiple of replacement length"
  },
  {
    "objectID": "intro.html#interpreting-the-key-driver-results",
    "href": "intro.html#interpreting-the-key-driver-results",
    "title": "2  Key Drivers models",
    "section": "2.3 Interpreting the Key Driver Results",
    "text": "2.3 Interpreting the Key Driver Results\n\n2.3.1 Overview\nThe rescaled percentages in the output can be interpreted as the relative importance in moving 4’s to 5’s on OSAT for each of the measures, with the benefit of being able to sum the percentages into categories. The 3 categories are Service, Product, Environment. See the “Building a Key Driver Model” portion of this section for more info on the categories as well as what constitutes a good Key Driver model.\n\n\n2.3.2 Reading the Output - Correlations\nIf you ran the dropdown version of the syntax, as seen below, there is no formatting to your correlation table and it isn’t filtered to OSAT 4’s and 5’s. The Syntax version has formatting to match the SMart correlation table as well as the filter. Both work, the correlation table is to keep an eye on multicollinearity. Try to keep the correlations above .300 and below .800.\nIf you want to visualize the correlations in a more redable way, we recommend using corrplot like this:\n\nudf_csi_kd%>%\n  cor()%>%\n  corrplot(\"number\")\n\n\n\n\n\n\n2.3.3 Reading the Output - Descriptives\nThe Descriptives command gives you a quick high-level look at the R variables included in the analysis, you can check this to see if any of them have extremely low sample. We strive to keep 80% of the response base to keep the model representative.\n\n\n2.3.4 Reading the Output – Relative Weights & R\nExtracting the results from the object we just created is fairly easy, you can simply enter the name of the object into the console (udf_kd_model in this case).\n\nudf_kd_model\n\n[[1]]\n  Variable Names            Variable Labels Raw Weights (epsilon)\n1        R000017                      Speed            0.11317217\n2        R000029   Availability of Products            0.10341414\n3        R000022 Attentiveness of Associate            0.10187778\n4        R000020  Friendliness of Associate            0.09723517\n5        R000018       Exterior Cleanliness            0.09191880\n  Rescaled Weights p Value\n1        0.2229475       0\n2        0.2037243       0\n3        0.2006977       0\n4        0.1915518       0\n5        0.1810787       0\n\n[[2]]\n     R Squared Sample Size\n[1,] 0.5076181       26151\n\n\nAlso, if you want to see the tables specifically so you can paste them somewhere else, you can use the gt package like this:\nThese two tables give you the data needed to populate the Key Driver slide. You will use the Rescaled Weights from the far left column, which sum to 100%, in the Key Driver graph. In addition, your R2 is included in the last data table, the average R2 is .4 across clients. The R2 tells us how much variance in OSAT is explained by the metrics included in our model. We go more in depth on this topic in the Building Key Driver Models portion of this section. Lastly, your N count is included in the final table as well.\nResearch usually pastes the results into an excel, and sorts the variables based on the Rescaled Weights column."
  },
  {
    "objectID": "intro.html#building-a-key-driver-model",
    "href": "intro.html#building-a-key-driver-model",
    "title": "2  Key Drivers models",
    "section": "2.4 Building a Key Driver Model",
    "text": "2.4 Building a Key Driver Model\n\n2.4.1 Overview\nBuilding a Key Driver model is a bit like combining Science and Art. We have standard guidelines on how to build a model, but it takes understanding the client & industry as well statistical rigor to have a solid finished product. There are four steps to building a KD model: conceptualizing the model, sample considerations, choosing the measures, and validating. This portion of the training manual will walk through each of those steps. Our final goal is to have a “good” key driver model.\nWhat are the characteristics of a good key driver model?\n\nBalanced: There should be an equal number of similar questions\nDescriptive: Questions that provide clear indication of what is measured\nActionable: Questions that provide direction on what to change\nUnique: Questions that are not highly correlated with other questions in the model\nPredictive: Questions that have a meaningful impact on Overall Satisfaction\nRepresentative: The model should represent the intended customer base\n\n\n\n2.4.2 Step 1: Conceptualizing the KD Model\n\nUnderstand the Client’s Business:\n\nTalk to the account manager on the client to get an idea of what the business is and who goes there, in addition, taking the time to google the client on the internet to get an idea what the physical properties of the units look like can be useful too.\nIn addition, use the survey script in order to get a list of all satisfaction measures asked of respondents for your potential KD model. We only use 5-scale questions in key driver models. Also, pay attention to logic used in asking questions – we want to include measures that are asked of the majority of respondents (~80% of total respondents) to keep the model representative. An example would be Cleanliness of Restroom, it’s only asked of those who use the restroom which is typically less than half of respondents.\n\nBuilding a Balanced Model:\nKD Measures usually fall into one of three larger themes: Product, Service (sometimes called People), and Environment. Product related measures are typically like taste of food, or quality of merchandise. Service related measures are typically like Friendliness or Availability of Assistance. Environment measures are the least frequent and are typically like Cleanliness or Ease of Moving through Store. We try to not have too many drivers falling under one theme to keep a balance. This can be assessed by a simple count of measures under each theme or by adding the rescaled weights to get a picture of how influential that theme is.\n\n\n\n2.4.3 Step 2: Sample Considerations\n\nKey Driver models should have a total sample count of at least 1,000 responses\n\nA secondary threshold of 100 responses per question asked can be applied to models that fail the initial sample requirement\nAs we look at only 4’s and 5’s, you’ll want your client to ideally have around 1,500 responses before you run a model\n\nKeep sample in mind when choosing which measures to include\n\nIf a question isn’t asked of everyone, it could drastically cut down on sample and may not be representative of the experience\nIf it’s important to include a measure that has logic, consider whether it makes sense to have more than one model for the business. This is common for different lines of business within a retail client (ex. buy online and pick up in store) or dine type for restaurant clients.\nTry and keep the model representative with 80% of respondents included if a question with logic is included in the model\n\n\n\n\n2.4.4 Step 3: Choosing Measures\n\nIdeally, your model would include at least 5 measures and less than 10 measures. Not all possibly key driver metrics need to be included in the model.\nAfter conceptualizing the model and choosing measures to possibly include, check for high correlations across measures\n\nMeasures should not be included if they are correlated at .8 or higher with another measure\n\nAre all measures actionable at the unit level?\n\nTry to make sure your final model has mostly actionable measures. If the unit level user does not have control over the measure, like Variety of Menu, possibly consider not including it in the model.\nIf the measure is important to the model and the brand, consider removing Variety of Menu from the AFFs for your client. See Section 9: Areas for Focus for more information on this topic.\n\nYou may need to swap out measures if they are highly correlated to determine which measure should be included in the model\n\nKeep in mind actionability, balancing the model, what is important to the client, etc.\nDiscuss alternative models with your team if there are measures that could be swapped out for one another\n\nBe able to provide a recommendation to your team and be able to explain why measures were either added or removed.\n\nDocumentation is important, consider saving the multiple models you run before your final into an excel with descriptions of why you choose to add/remove measures\n\nDo not include outcome measures such as Overall Value, Return or Recommend in your model.\n\n\n\n2.4.5 Step 4: Validating\n\nAfter checking for multi-collinearity across measures, run the analysis to assess the strength of the model\n\nR-squared averages around .4 and is used to explain how much variation in moving 4s to 5s is explained by the measures included. We aim for an R-squared of .5\nModels at or below .3 should be investigated to understand if measures should be added/removed in order to bring the model in with best practices\n\nCheck the sample size to ensure that the model has been run correctly (should match a list-wise correlation table filtered for 4s and 5s)\nMake sure the top drivers make sense for what you would expect for the client’s business\nWhen creating a key driver model, it is useful to also run a Rewards and Penalties model to see how they compare. We’ll cover how to run Rewards and Penalties next.\n\nTop drivers are not always the biggest penalties or rewards, but should still be important in the model\nKey driver models usually differ from the Rewards and Penalties model. A Rewards and Penalties model shows which measures provide the biggest penalties and which measures provide the biggest rewards.\nThe model uses either linear or relative-weights linear regression to look at how variation in Overall Satisfaction is explained by either scoring top box or bottom 3 box on a measure\n\n\n\n\n2.4.6 Final Notes\nWe try to have only one KD model per client, but occasionally it makes sense to have alternative or additional models. One example could be Drive-thru vs Dine In for restaurants. Or, another example for retail could be Buy Online/Pick Up in Store Shoppers vs Regular Shoppers. The same criteria apply to a filtered key driver model as to the overall model. It is easy to run into low n counts or a weak R squared in these alternative models. If the model cannot stand on its own, it should not be shown to a client."
  },
  {
    "objectID": "correlations.html",
    "href": "correlations.html",
    "title": "1  Correlations",
    "section": "",
    "text": "For this example we are going to use some NAPA_PSHIP data, if you want to replicate the analyses, you can pull the data using the hiden code\nInside R, there are multiple ways to run correlations, some from Base R and some from other very useful external packages.\nThe easiest way to compute a correlation between two variables is to use the cor.test function from the stats package which is automatically loaded in any R session. This function takes as input two vectors of the same size and it assumes they are both ordered in the same way, meaning that row 1 in the first vector corresponds to row 1 in the second vector.\nSo, to extract the vectors from an existing data frame, we would use the $ operator like this cx_data$var1. Putting it inside the function would look something like this:\nWhen you run it just like that, by default it will give you the text based results that you can see above. However, if you want to use the results later, it might be useful to store them in a data frame. To do so, the tidy() function from the broom package can be handy.\nThis way, you could export the data frame into an excel using openxlsx or to an HTML table using gt() or datatable() from the DT package."
  },
  {
    "objectID": "correlations.html#matrix-of-correlations",
    "href": "correlations.html#matrix-of-correlations",
    "title": "1  Correlations",
    "section": "1.1 Matrix of correlations",
    "text": "1.1 Matrix of correlations\nHowever, if you are trying to see the correlations between multiple variables, the cor.test() function might not be the best choice for you. Rather, the easiest way to compute a correlation matrix is using the cor() function also from the stats package. This function takes an input a numerical data frame, meaning that we first need to filter out for just the variables we want to analyze. Fortunately, using the base R (|>) or the tidyverse pipe (%>%) makes it easy enough. You can do something like this:\n\n\nShow the code\n cx_data%>%\n  select(R003000_Overall_Satisfaction:R000012_Overall_Value)%>%\n  drop_na()%>%\n  cor()\n\n\n                                     R003000_Overall_Satisfaction\nR003000_Overall_Satisfaction                            1.0000000\nR000003_Delivery_Time                                   0.5961212\nR000004_Delivery_Driver_Friendliness                    0.1021728\nR000005_Product_Description                             0.5769551\nR000006_Product_Images                                  0.5340010\nR000007_Order_Accuracy                                  0.5827870\nR000009_Ease_of_NavigatingPA_PROLink                    0.6142104\nR000010_Ease_of_Placing_Order                           0.6317569\nR000011_Stock_Availability_Accuracy                     0.5970378\nR000012_Overall_Value                                   0.5641530\n                                     R000003_Delivery_Time\nR003000_Overall_Satisfaction                     0.5961212\nR000003_Delivery_Time                            1.0000000\nR000004_Delivery_Driver_Friendliness             0.1383463\nR000005_Product_Description                      0.4930749\nR000006_Product_Images                           0.4842543\nR000007_Order_Accuracy                           0.6145661\nR000009_Ease_of_NavigatingPA_PROLink             0.3699018\nR000010_Ease_of_Placing_Order                    0.4581992\nR000011_Stock_Availability_Accuracy              0.6251557\nR000012_Overall_Value                            0.5202923\n                                     R000004_Delivery_Driver_Friendliness\nR003000_Overall_Satisfaction                                   0.10217282\nR000003_Delivery_Time                                          0.13834625\nR000004_Delivery_Driver_Friendliness                           1.00000000\nR000005_Product_Description                                    0.13776047\nR000006_Product_Images                                         0.14625946\nR000007_Order_Accuracy                                         0.16959359\nR000009_Ease_of_NavigatingPA_PROLink                           0.08943948\nR000010_Ease_of_Placing_Order                                  0.12954312\nR000011_Stock_Availability_Accuracy                            0.13952907\nR000012_Overall_Value                                          0.11844941\n                                     R000005_Product_Description\nR003000_Overall_Satisfaction                           0.5769551\nR000003_Delivery_Time                                  0.4930749\nR000004_Delivery_Driver_Friendliness                   0.1377605\nR000005_Product_Description                            1.0000000\nR000006_Product_Images                                 0.7736078\nR000007_Order_Accuracy                                 0.6509746\nR000009_Ease_of_NavigatingPA_PROLink                   0.5634418\nR000010_Ease_of_Placing_Order                          0.5900835\nR000011_Stock_Availability_Accuracy                    0.6111687\nR000012_Overall_Value                                  0.5652662\n                                     R000006_Product_Images\nR003000_Overall_Satisfaction                      0.5340010\nR000003_Delivery_Time                             0.4842543\nR000004_Delivery_Driver_Friendliness              0.1462595\nR000005_Product_Description                       0.7736078\nR000006_Product_Images                            1.0000000\nR000007_Order_Accuracy                            0.6305591\nR000009_Ease_of_NavigatingPA_PROLink              0.5460741\nR000010_Ease_of_Placing_Order                     0.5747142\nR000011_Stock_Availability_Accuracy               0.5879939\nR000012_Overall_Value                             0.5502345\n                                     R000007_Order_Accuracy\nR003000_Overall_Satisfaction                      0.5827870\nR000003_Delivery_Time                             0.6145661\nR000004_Delivery_Driver_Friendliness              0.1695936\nR000005_Product_Description                       0.6509746\nR000006_Product_Images                            0.6305591\nR000007_Order_Accuracy                            1.0000000\nR000009_Ease_of_NavigatingPA_PROLink              0.4539392\nR000010_Ease_of_Placing_Order                     0.5399908\nR000011_Stock_Availability_Accuracy               0.6453447\nR000012_Overall_Value                             0.5236993\n                                     R000009_Ease_of_NavigatingPA_PROLink\nR003000_Overall_Satisfaction                                   0.61421040\nR000003_Delivery_Time                                          0.36990183\nR000004_Delivery_Driver_Friendliness                           0.08943948\nR000005_Product_Description                                    0.56344180\nR000006_Product_Images                                         0.54607410\nR000007_Order_Accuracy                                         0.45393918\nR000009_Ease_of_NavigatingPA_PROLink                           1.00000000\nR000010_Ease_of_Placing_Order                                  0.71653113\nR000011_Stock_Availability_Accuracy                            0.47663870\nR000012_Overall_Value                                          0.50967315\n                                     R000010_Ease_of_Placing_Order\nR003000_Overall_Satisfaction                             0.6317569\nR000003_Delivery_Time                                    0.4581992\nR000004_Delivery_Driver_Friendliness                     0.1295431\nR000005_Product_Description                              0.5900835\nR000006_Product_Images                                   0.5747142\nR000007_Order_Accuracy                                   0.5399908\nR000009_Ease_of_NavigatingPA_PROLink                     0.7165311\nR000010_Ease_of_Placing_Order                            1.0000000\nR000011_Stock_Availability_Accuracy                      0.5206984\nR000012_Overall_Value                                    0.5128571\n                                     R000011_Stock_Availability_Accuracy\nR003000_Overall_Satisfaction                                   0.5970378\nR000003_Delivery_Time                                          0.6251557\nR000004_Delivery_Driver_Friendliness                           0.1395291\nR000005_Product_Description                                    0.6111687\nR000006_Product_Images                                         0.5879939\nR000007_Order_Accuracy                                         0.6453447\nR000009_Ease_of_NavigatingPA_PROLink                           0.4766387\nR000010_Ease_of_Placing_Order                                  0.5206984\nR000011_Stock_Availability_Accuracy                            1.0000000\nR000012_Overall_Value                                          0.5794517\n                                     R000012_Overall_Value\nR003000_Overall_Satisfaction                     0.5641530\nR000003_Delivery_Time                            0.5202923\nR000004_Delivery_Driver_Friendliness             0.1184494\nR000005_Product_Description                      0.5652662\nR000006_Product_Images                           0.5502345\nR000007_Order_Accuracy                           0.5236993\nR000009_Ease_of_NavigatingPA_PROLink             0.5096731\nR000010_Ease_of_Placing_Order                    0.5128571\nR000011_Stock_Availability_Accuracy              0.5794517\nR000012_Overall_Value                            1.0000000\n\n\nThe only things we did was selecting the variables we wanted and then making sure that there were no NA values in them by droping any row with NAs. However, if you want to do correlations using pariwise complete observations, you can adjust that inside the cor() call like this: cor(data, use = \"pairwise.complete.obs\"), if you want to check what other options are available, you can always use ?cor to display the documentation of the given function.\nThis is a very easy way to compute a correlation matrix, but it has two main issues that we can address using additional steps from other packages. The first issue is that again, it is not very readable nor exportable. The simplest fix is to transform the matrix that the function outputs into a data frame keeping the row names. To do so, we use the base R as.data.frame() and rownames_to_column() from the tibble package.\n\n\nShow the code\ncorrelation_matrix_cx <- cx_data%>%\n  select(R003000_Overall_Satisfaction:R000012_Overall_Value)%>%\n  drop_na()%>%\n  cor()%>%\n  as.data.frame()%>%\n  tibble::rownames_to_column() \n\n\nOnce we have it like that we can go ahead and export in whatever way we prefer. For example, if we want to use an HTML output, we can color code the results directly from the gt package set of functions, doing something like this:\n\n\nShow the code\nlibrary(gt)\ncorrelation_matrix_cx%>% \n  gt(rowname_col = \"rowname\")%>%\n  tab_style_body(columns = starts_with(\"R0\"),fn = function(x) between(x, .7, .99),\n    style = cell_text(color = 'red', weight = 'bold'))\n\n\n\n\n\n\n  \n    \n    \n      \n      R003000_Overall_Satisfaction\n      R000003_Delivery_Time\n      R000004_Delivery_Driver_Friendliness\n      R000005_Product_Description\n      R000006_Product_Images\n      R000007_Order_Accuracy\n      R000009_Ease_of_NavigatingPA_PROLink\n      R000010_Ease_of_Placing_Order\n      R000011_Stock_Availability_Accuracy\n      R000012_Overall_Value\n    \n  \n  \n    R003000_Overall_Satisfaction\n1.0000000\n0.5961212\n0.10217282\n0.5769551\n0.5340010\n0.5827870\n0.61421040\n0.6317569\n0.5970378\n0.5641530\n    R000003_Delivery_Time\n0.5961212\n1.0000000\n0.13834625\n0.4930749\n0.4842543\n0.6145661\n0.36990183\n0.4581992\n0.6251557\n0.5202923\n    R000004_Delivery_Driver_Friendliness\n0.1021728\n0.1383463\n1.00000000\n0.1377605\n0.1462595\n0.1695936\n0.08943948\n0.1295431\n0.1395291\n0.1184494\n    R000005_Product_Description\n0.5769551\n0.4930749\n0.13776047\n1.0000000\n0.7736078\n0.6509746\n0.56344180\n0.5900835\n0.6111687\n0.5652662\n    R000006_Product_Images\n0.5340010\n0.4842543\n0.14625946\n0.7736078\n1.0000000\n0.6305591\n0.54607410\n0.5747142\n0.5879939\n0.5502345\n    R000007_Order_Accuracy\n0.5827870\n0.6145661\n0.16959359\n0.6509746\n0.6305591\n1.0000000\n0.45393918\n0.5399908\n0.6453447\n0.5236993\n    R000009_Ease_of_NavigatingPA_PROLink\n0.6142104\n0.3699018\n0.08943948\n0.5634418\n0.5460741\n0.4539392\n1.00000000\n0.7165311\n0.4766387\n0.5096731\n    R000010_Ease_of_Placing_Order\n0.6317569\n0.4581992\n0.12954312\n0.5900835\n0.5747142\n0.5399908\n0.71653113\n1.0000000\n0.5206984\n0.5128571\n    R000011_Stock_Availability_Accuracy\n0.5970378\n0.6251557\n0.13952907\n0.6111687\n0.5879939\n0.6453447\n0.47663870\n0.5206984\n1.0000000\n0.5794517\n    R000012_Overall_Value\n0.5641530\n0.5202923\n0.11844941\n0.5652662\n0.5502345\n0.5236993\n0.50967315\n0.5128571\n0.5794517\n1.0000000\n  \n  \n  \n\n\n\n\nThis looks better as is, and most of the time it should be enough, but the other potential issue we have with this approach is that it doesn’t give us any information on the significance of the correlation, the confidence intervals or the method used to extract the coefficient. To get those, a very nice wrapper is the correlation() function from the package with the same name. It automatically gives out a tidy table like the one we accomplished with cor.test() but for multiple pairs of correlations. To achieve this, you can simply replace the cor() call from the prior calls and use correlation instead. ## Correlation pairs\n\n\nShow the code\nlibrary(correlation)\n\n    cx_data%>%\n      select(R003000_Overall_Satisfaction:R000012_Overall_Value)%>%\n      correlation::correlation()%>%\n      datatable()\n\n\n\n\n\n\n\nA nice feature of this function is that it allows to keep all redundant pairs, which makes it easy to filter for just one variable on one column so we can see its correlation to the rest of the variables instead of seeing all pairs. For example, if we are interestes in seeing how all possible key drivers correlate to OSAT, we can simply add another pipe and filter for OSAT in the first column\n\n\nShow the code\n   cx_data%>%\n      select(R003000_Overall_Satisfaction:R000012_Overall_Value)%>%\n      correlation::correlation(redundant = T)%>%\n  filter(Parameter1 == \"R003000_Overall_Satisfaction\")%>%\n      datatable()"
  },
  {
    "objectID": "correlations.html#corrplot",
    "href": "correlations.html#corrplot",
    "title": "1  Correlations",
    "section": "1.2 Corrplot",
    "text": "1.2 Corrplot\nFinally as a bonus option to display correlations, we can use the package and function corrplot that allows to show correlations in a more visual way. The base function simply shows correlations in a color gradient. The main input it needs is a correlation matrix, so we can simply use the code we created before up until the cor() call and then simply add a call for the corrplot() function.\n\n\nShow the code\nlibrary(corrplot)\ncx_data%>%\n  select(R003000_Overall_Satisfaction:R000012_Overall_Value)%>%\n  drop_na()%>%\n  cor()%>%\n  corrplot()\n\n\n\n\n\nThis package has a lot of features that can get you to very advances forms of displaying not only correlations but any other types of matrices. I’d recommend you to check for more details in the following link:\n\nPro tip:\n\nOne issue with this function is that, since it shows vertically and horizontally the names of all your vars, given the name coding we tend to use here at SMG, this causes the graph to be very smalled compared to the text. There are few workarounds related to the package itself such as the tl.srt or tl.cex arguments inside the corrplot function, but also, another option is renaming all our variables into a nicer cleaner way. If we are only using vars starting with R0... we can simply use the code I show in the next box to get rid of the FNS and simply keep the name of each variable.\n\n\nShow the code\ncorrplot_data <- cx_data%>%  \n  select(UID, R003000_Overall_Satisfaction:R000012_Overall_Value)%>%\n  pivot_longer(-UID, names_to = \"var\")%>%\n  separate(var, into = c(\"FNS\", \"var\"), sep = \"_\", extra = \"merge\")%>%\n  select(-FNS)%>%\n  pivot_wider(names_from = \"var\", values_from = \"value\")%>%\n  select(-UID)%>%\n  drop_na()%>%\n  cor()\n  \ncorrplot_data%>%\n  corrplot()  \n\n\n\n\n\nLooks better, but just to get you a gist of what this package is capable of, here is an example of what a final clean corrplot can look like with a few very simple tweaks\n\n\nShow the code\n  corrplot_data%>%\n      corrplot(method = \"number\",type = 'lower',tl.col = 'black',\n         cl.ratio = .1, tl.srt = 45,tl.cex = 0.5, number.cex=0.7)"
  },
  {
    "objectID": "key_drivers.html",
    "href": "key_drivers.html",
    "title": "2  Key Drivers models",
    "section": "",
    "text": "“Multiple regression is perhaps the most frequently used statistical tool for the analysis of data in the organizational sciences. The information provided by such analyses is particularly useful for addressing issues related to prediction such as identifying a set of predictors that will maximize the amount of variance explained in the criterion. However, most researchers and practitioners are simultaneously interested in multiple regression for theory testing or explanation purposes. Here, the question of interest becomes understanding the extent to which each variable drives the prediction. Essentially, one wishes to understand the contribution each predictor makes towards explaining variance in the criterion. Past research has documented how indices commonly produced by multiple regression analyses fail to appropriately partition variance to the various predictors when they are correlated (Darlington 1968). In response, two alternative approaches, dominance analysis (Budescu 1993) and relative weight analysis (Fabbris 1980; Johnson 2000), have been developed that allow for more accurate variance partitioning among correlated predictors.”\nFrom “Relative Importance Analysis: A Useful Supplement to Regression Analysis” – found here\nRelative weights analysis uses the correlations between the dependent and independent variables and determines what percentage of the explained variance can be attributed back to each independent variable. The benefit to relative weights is that these percentages are additive, meaning that you can sum up the percentages to create groups with similar themes. For instance, questions about the helpfulness, friendliness, and knowledge of associates can be grouped to create an associates factor.\n\n\n\nRelative weights are specifically developed for use with correlated predictors\nConsiders the predictive value of the variable both in isolation and in the presence of other variables\nThe non-scaled weights sum to the R-squared and thus can be used as estimates of relative effect sizes\nRelative weights are easily rescaled into proportions or percentages which is easily communicated to clients\n\n\n\n\nKey Driver Analyses are the key deliverable and the bread-and-butter of the Research Department. With key driver analysis, we are concerned with evaluating the relative importance of different aspects of the customer’s product and service experience in predicting their Overall Satisfaction.\nThe Relative Weights analysis works very well with the types of variables we utilize in our analyses at SMG, as they are usually very highly correlated. When we run a linear regression, it typically results in one variable taking the lion’s share of variance as the top driver, while the rest fall in at a much smaller variance due to the nature of linear regressions. Using Relative Weights allows us to assign that shared variance back to the original variables, which results in a much more balanced predictive model. In addition, it’s easier for clients to understand as the rescaled importance weights sum to 100%.\nOur Key Driver models are filtered down to only 4’s & 5’s (or satisfied and highly satisfied) for Overall Satisfaction. We do this to hone in on what is most important to driving loyalty. When we look at our loyalty curve crosstabs (% Highly Likely to Return or Recommend broken out by Overall Satisfaction rating), we see that moving a 4 (Satisfied) on OSAT to a 5 (Highly Satisfied) on OSAT results in a two to three times greater % Highly Likely to Return or Recommend. In addition, normally about two thirds of guests fall into these two buckets, so it’s very representative of the survey guest base. If a client is interested in a full-scale approach, consider running Rewards & Penalties, covered later in this section as well as in section 8 of the training manual “SMart Gen II”, Analyses: Key Driver (Relative Weights)."
  },
  {
    "objectID": "key_drivers.html#running-the-key-driver-analysis",
    "href": "key_drivers.html#running-the-key-driver-analysis",
    "title": "2  Key Drivers models",
    "section": "2.2 Running the Key Driver Analysis",
    "text": "2.2 Running the Key Driver Analysis\n\n2.2.1 Materials & Applications to run Key Drivers\nThe Key Driver analysis has been automated to run in SMart, see section 8, Analyses: Key Driver (Relative Weights) Analysis for further information. However, we still occasionally run Key Drivers manually in R via the SMG package using the key_driver() function.\n\n\n2.2.2 Pull the data\nFor this example we are going to use data from United Dairy Farms. To pull the data directly from SQL we use the following syntax:\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(odbc)\nlibrary(corrplot)\nlibrary(papeR)\nlibrary(gt)\nlibrary(broom)\nconnection <- dbConnect(odbc(),\n                        Driver = \"SQL Server\",\n                        Server = \"SRV_INT_TRANS\\\\SMGTRANS\",\n                        Trusted_Connection = \"True\")\n\nudf_csi_kd <- dbGetQuery(connection,\n\"SELECT \nUDF_CSI_Calls.[UID], \nUDF_CSI_Calls.[R000005], \nUDF_CSI_Calls.[R000017], \nUDF_CSI_Calls.[R000018], \nUDF_CSI_Calls.[R000020], \nUDF_CSI_Calls.[R000022], \nUDF_CSI_Calls.[R000029]\nFROM \nedify.dbo.UDF_CSI_Calls WITH (NOLOCK)  \nINNER JOIN structures.dbo.UDF_CSI_Store_Info WITH (NOLOCK) ON UDF_CSI_Calls.[StoreId] = UDF_CSI_Store_Info.[StoreId] \nWHERE \nUDF_CSI_Store_Info.[IN_CSI] = -1 AND UDF_CSI_Store_Info.[TEST_STORE] = 0\nAND UDF_CSI_Calls.[Date_Time] BETWEEN '2022-01-01 12:00:00 AM' AND '2022-12-31 11:59:59 PM'\nAND UDF_CSI_Calls.[TERM] IN ('OK', 'WebOK')\")\n\n\nIn this case we only pull the variables that we need for the analysis.\n\n\n2.2.3 Prepare data\nIn order to get the data into the right format, it is important to perform the following steps:\n\nSelect only the variables that we are going to use including OSAT and the explanatory variables (make sure OSAT is the first variable in the dataset)\nKeep only complete cases\nFilter OSAT so it only includes 4s and 5s\n\n-Convert 4s into 0s and 5s into 1s\nAll those steps can be performed in a single run of code using the tidyverse like this:\n\n\nShow the code\nudf_csi_kd <- udf_csi_kd%>%\n  select(-UID)%>%\n  filter_all(all_vars(.%in% 1:5))%>%\n  filter(R000005 %in% 4:5)%>%\n  mutate(R000005 = case_when(R000005 == 4 ~ 0, R000005 == 5 ~ 1))\n\n\n\n\n2.2.4 Get correlations\nThe easiest way to run simple correlations between the data is using the following code:\n\n\nShow the code\nudf_csi_kd%>%\n  cor()\n\n\n          R000005   R000017   R000018   R000020   R000022   R000029\nR000005 1.0000000 0.5833673 0.5206141 0.5725330 0.5933566 0.5278057\nR000017 0.5833673 1.0000000 0.5553600 0.6268914 0.6799234 0.5358979\nR000018 0.5206141 0.5553600 1.0000000 0.5502581 0.5876474 0.5350537\nR000020 0.5725330 0.6268914 0.5502581 1.0000000 0.7743361 0.4975037\nR000022 0.5933566 0.6799234 0.5876474 0.7743361 1.0000000 0.5410453\nR000029 0.5278057 0.5358979 0.5350537 0.4975037 0.5410453 1.0000000\n\n\n\n\n2.2.5 Get descriptives\n\n\n2.2.6 Run key drivers function\nThe key drivers function is built so it only needs two main inputs, the dataset itself and the name of the project. Once all the previous steps have been performed, you can easily run the model with these lines of code\n\n\nShow the code\nudf_kd_model <- udf_csi_kd%>%\n  key_drivers(\"UDF_CSI\")"
  },
  {
    "objectID": "key_drivers.html#interpreting-the-key-driver-results",
    "href": "key_drivers.html#interpreting-the-key-driver-results",
    "title": "2  Key Drivers models",
    "section": "2.3 Interpreting the Key Driver Results",
    "text": "2.3 Interpreting the Key Driver Results\n\n2.3.1 Overview\nThe rescaled percentages in the output can be interpreted as the relative importance in moving 4’s to 5’s on OSAT for each of the measures, with the benefit of being able to sum the percentages into categories. The 3 categories are Service, Product, Environment. See the “Building a Key Driver Model” portion of this section for more info on the categories as well as what constitutes a good Key Driver model.\n\n\n2.3.2 Reading the Output - Correlations\nIf you ran the dropdown version of the syntax, as seen below, there is no formatting to your correlation table and it isn’t filtered to OSAT 4’s and 5’s. The Syntax version has formatting to match the SMart correlation table as well as the filter. Both work, the correlation table is to keep an eye on multicollinearity. Try to keep the correlations above .300 and below .800.\nIf you want to visualize the correlations in a more redable way, we recommend using corrplot like this:\n\n\nShow the code\nudf_csi_kd%>%\n  cor()%>%\n  corrplot(\"number\")\n\n\n\n\n\n\n\n2.3.3 Reading the Output - Descriptives\nThe Descriptives command gives you a quick high-level look at the R variables included in the analysis, you can check this to see if any of them have extremely low sample. We strive to keep 80% of the response base to keep the model representative.\n\n\n2.3.4 Reading the Output – Relative Weights & R\nExtracting the results from the object we just created is fairly easy, you can simply enter the name of the object into the console (udf_kd_model in this case).\n\n\nShow the code\nudf_kd_model\n\n\n[[1]]\n  Variable Names            Variable Labels Raw Weights (epsilon)\n1        R000017                      Speed            0.11317217\n2        R000029   Availability of Products            0.10341414\n3        R000022 Attentiveness of Associate            0.10187778\n4        R000020  Friendliness of Associate            0.09723517\n5        R000018       Exterior Cleanliness            0.09191880\n  Rescaled Weights p Value\n1        0.2229475       0\n2        0.2037243       0\n3        0.2006977       0\n4        0.1915518       0\n5        0.1810787       0\n\n[[2]]\n     R Squared Sample Size\n[1,] 0.5076181       26151\n\n\nAlso, if you want to see the tables specifically so you can paste them somewhere else, you can use the gt package like this:\nThese two tables give you the data needed to populate the Key Driver slide. You will use the Rescaled Weights from the far left column, which sum to 100%, in the Key Driver graph. In addition, your R2 is included in the last data table, the average R2 is .4 across clients. The R2 tells us how much variance in OSAT is explained by the metrics included in our model. We go more in depth on this topic in the Building Key Driver Models portion of this section. Lastly, your N count is included in the final table as well.\nResearch usually pastes the results into an excel, and sorts the variables based on the Rescaled Weights column."
  },
  {
    "objectID": "key_drivers.html#building-a-key-driver-model",
    "href": "key_drivers.html#building-a-key-driver-model",
    "title": "2  Key Drivers models",
    "section": "2.4 Building a Key Driver Model",
    "text": "2.4 Building a Key Driver Model\n\n2.4.1 Overview\nBuilding a Key Driver model is a bit like combining Science and Art. We have standard guidelines on how to build a model, but it takes understanding the client & industry as well statistical rigor to have a solid finished product. There are four steps to building a KD model: conceptualizing the model, sample considerations, choosing the measures, and validating. This portion of the training manual will walk through each of those steps. Our final goal is to have a “good” key driver model.\nWhat are the characteristics of a good key driver model?\n\nBalanced: There should be an equal number of similar questions\nDescriptive: Questions that provide clear indication of what is measured\nActionable: Questions that provide direction on what to change\nUnique: Questions that are not highly correlated with other questions in the model\nPredictive: Questions that have a meaningful impact on Overall Satisfaction\nRepresentative: The model should represent the intended customer base\n\n\n\n2.4.2 Step 1: Conceptualizing the KD Model\n\nUnderstand the Client’s Business:\n\nTalk to the account manager on the client to get an idea of what the business is and who goes there, in addition, taking the time to google the client on the internet to get an idea what the physical properties of the units look like can be useful too.\nIn addition, use the survey script in order to get a list of all satisfaction measures asked of respondents for your potential KD model. We only use 5-scale questions in key driver models. Also, pay attention to logic used in asking questions – we want to include measures that are asked of the majority of respondents (~80% of total respondents) to keep the model representative. An example would be Cleanliness of Restroom, it’s only asked of those who use the restroom which is typically less than half of respondents.\n\nBuilding a Balanced Model:\nKD Measures usually fall into one of three larger themes: Product, Service (sometimes called People), and Environment. Product related measures are typically like taste of food, or quality of merchandise. Service related measures are typically like Friendliness or Availability of Assistance. Environment measures are the least frequent and are typically like Cleanliness or Ease of Moving through Store. We try to not have too many drivers falling under one theme to keep a balance. This can be assessed by a simple count of measures under each theme or by adding the rescaled weights to get a picture of how influential that theme is.\n\n\n\n2.4.3 Step 2: Sample Considerations\n\nKey Driver models should have a total sample count of at least 1,000 responses\n\nA secondary threshold of 100 responses per question asked can be applied to models that fail the initial sample requirement\nAs we look at only 4’s and 5’s, you’ll want your client to ideally have around 1,500 responses before you run a model\n\nKeep sample in mind when choosing which measures to include\n\nIf a question isn’t asked of everyone, it could drastically cut down on sample and may not be representative of the experience\nIf it’s important to include a measure that has logic, consider whether it makes sense to have more than one model for the business. This is common for different lines of business within a retail client (ex. buy online and pick up in store) or dine type for restaurant clients.\nTry and keep the model representative with 80% of respondents included if a question with logic is included in the model\n\n\n\n\n2.4.4 Step 3: Choosing Measures\n\nIdeally, your model would include at least 5 measures and less than 10 measures. Not all possibly key driver metrics need to be included in the model.\nAfter conceptualizing the model and choosing measures to possibly include, check for high correlations across measures\n\nMeasures should not be included if they are correlated at .8 or higher with another measure\n\nAre all measures actionable at the unit level?\n\nTry to make sure your final model has mostly actionable measures. If the unit level user does not have control over the measure, like Variety of Menu, possibly consider not including it in the model.\nIf the measure is important to the model and the brand, consider removing Variety of Menu from the AFFs for your client. See Section 9: Areas for Focus for more information on this topic.\n\nYou may need to swap out measures if they are highly correlated to determine which measure should be included in the model\n\nKeep in mind actionability, balancing the model, what is important to the client, etc.\nDiscuss alternative models with your team if there are measures that could be swapped out for one another\n\nBe able to provide a recommendation to your team and be able to explain why measures were either added or removed.\n\nDocumentation is important, consider saving the multiple models you run before your final into an excel with descriptions of why you choose to add/remove measures\n\nDo not include outcome measures such as Overall Value, Return or Recommend in your model.\n\n\n\n2.4.5 Step 4: Validating\n\nAfter checking for multi-collinearity across measures, run the analysis to assess the strength of the model\n\nR-squared averages around .4 and is used to explain how much variation in moving 4s to 5s is explained by the measures included. We aim for an R-squared of .5\nModels at or below .3 should be investigated to understand if measures should be added/removed in order to bring the model in with best practices\n\nCheck the sample size to ensure that the model has been run correctly (should match a list-wise correlation table filtered for 4s and 5s)\nMake sure the top drivers make sense for what you would expect for the client’s business\nWhen creating a key driver model, it is useful to also run a Rewards and Penalties model to see how they compare. We’ll cover how to run Rewards and Penalties next.\n\nTop drivers are not always the biggest penalties or rewards, but should still be important in the model\nKey driver models usually differ from the Rewards and Penalties model. A Rewards and Penalties model shows which measures provide the biggest penalties and which measures provide the biggest rewards.\nThe model uses either linear or relative-weights linear regression to look at how variation in Overall Satisfaction is explained by either scoring top box or bottom 3 box on a measure\n\n\n\n\n2.4.6 Final Notes\nWe try to have only one KD model per client, but occasionally it makes sense to have alternative or additional models. One example could be Drive-thru vs Dine In for restaurants. Or, another example for retail could be Buy Online/Pick Up in Store Shoppers vs Regular Shoppers. The same criteria apply to a filtered key driver model as to the overall model. It is easy to run into low n counts or a weak R squared in these alternative models. If the model cannot stand on its own, it should not be shown to a client."
  },
  {
    "objectID": "quick_r_tips.html",
    "href": "quick_r_tips.html",
    "title": "3  Quick R tips",
    "section": "",
    "text": "Show the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "date_times.html",
    "href": "date_times.html",
    "title": "3  Handling Times and Dates",
    "section": "",
    "text": "Our calls table has VisitDate and Date_Time already formatted as date and datetime objects, which can be really handy for some of the analyses can run in R related to time and dates. For that our main ally will be the lubridate package, aside from the fact that it integrates very well with tidyverse syntax it is an overall really powerfull package to work with dates and time, and its syntax is fairly straightforward. In this section we will cover the following analyses:\nIf you are interested in learning more about lubridate, I’d recommend to check out this [[resource]]"
  },
  {
    "objectID": "date_times.html#data-pull",
    "href": "date_times.html#data-pull",
    "title": "3  Handling Times and Dates",
    "section": "3.1 Data pull",
    "text": "3.1 Data pull\nFor this example we are going to use data from Shell Turkey. The data pull includes the Market filter inside the SQL call, if you are interested in learning more about these custom data pulls, please visit the chapter we have on SQL.\n\n\nShow the code\nlibrary(odbc)\n\n\nconnection <- dbConnect(\n  odbc(), \n  Driver = 'SQL Server', \n  Server = 'SRV_INT_TRANS\\\\SMGTRANS',\n  Trusted_Connection = 'True')\n\n\n## Create the SQL string to pull the data\nshel_csi_query <- \"SELECT\nSHEL_CSI_Calls.[UID], \nSHEL_CSI_Calls.[StoreId], \nSHEL_CSI_Calls.[VisitDate], \nSHEL_CSI_Calls.[R003000] \n\nFROM\nedify.dbo.SHEL_CSI_Calls WITH (NOLOCK)  \nINNER JOIN structures.dbo.SHEL_CSI_Store_Info WITH (NOLOCK) ON SHEL_CSI_Calls.[StoreId] = SHEL_CSI_Store_Info.[StoreId]\nINNER join structures.dbo.SHEL_CSI_Market_Info AS SHEL_CSI_Market_Info(nolock) ON SHEL_CSI_Store_Info.MarketID = SHEL_CSI_Market_Info.MarketID\nWHERE\nSHEL_CSI_Store_Info.[IN_CSI] = -1 AND SHEL_CSI_Store_Info.[TEST_STORE] = 0\nAND SHEL_CSI_Calls.[VisitDate] BETWEEN '2023-04-01 12:00:00 AM' AND '2023-09-30 11:59:59 PM' AND\n                       SHEL_CSI_Calls.[TERM] IN ('WebOk', 'OK')\nAND SHEL_CSI_Market_Info.[MarketName] = 'Turkey'\"\n\n\n## Pull the data\nshel_csi <- dbGetQuery(connection, shel_csi_query)"
  },
  {
    "objectID": "date_times.html#trends-by-month-week-or-day",
    "href": "date_times.html#trends-by-month-week-or-day",
    "title": "3  Handling Times and Dates",
    "section": "3.2 Trends by month, week or day",
    "text": "3.2 Trends by month, week or day\nTo create trends at different time levels, the lubridate package makes things easy with extracting functions such asmonth(), day(), week() and many others. What these functions do is basically extract a single component of a date variable. For example, if we want to know the trend of OSAT during all of 2023 we would need to do something like this:\n\n\nShow the code\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(tidyr)\n\nshel_csi%>%\n  mutate(month = month(VisitDate))%>%\n  group_by(month)%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n# A tibble: 6 x 3\n  month mean_osat count\n  <dbl>     <dbl> <int>\n1     4     0.599 36641\n2     5     0.618 29478\n3     6     0.612 42059\n4     7     0.565 39906\n5     8     0.551 40683\n6     9     0.570 39265\n\n\n\nPro tip #1: You can perform mutate like operations insde the group_by call, allowing you to save one line of code and perform both actions in the same call, the code would look something like this:\n\n\n\nShow the code\nshel_csi%>%\n  group_by(month = month(VisitDate))%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n# A tibble: 6 x 3\n  month mean_osat count\n  <dbl>     <dbl> <int>\n1     4     0.599 36641\n2     5     0.618 29478\n3     6     0.612 42059\n4     7     0.565 39906\n5     8     0.551 40683\n6     9     0.570 39265\n\n\nIf you prefer to use the label for each month instead of a number, you can use the parameter label = TRUE inside the month() call like this:\n\n\nShow the code\nshel_csi%>%\n  group_by(month = month(VisitDate, label = T))%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n# A tibble: 6 x 3\n  month mean_osat count\n  <ord>     <dbl> <int>\n1 Apr       0.599 36641\n2 May       0.618 29478\n3 Jun       0.612 42059\n4 Jul       0.565 39906\n5 Aug       0.551 40683\n6 Sep       0.570 39265\n\n\nIn case you had included data for more than one year, you can always do a double group_by() so you can have both the year and the month as your groups like this:\n\n\nShow the code\nshel_csi%>%\n  group_by(year = year(VisitDate), month = month(VisitDate, label = T))%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n# A tibble: 6 x 4\n# Groups:   year [1]\n   year month mean_osat count\n  <dbl> <ord>     <dbl> <int>\n1  2023 Apr       0.599 36641\n2  2023 May       0.618 29478\n3  2023 Jun       0.612 42059\n4  2023 Jul       0.565 39906\n5  2023 Aug       0.551 40683\n6  2023 Sep       0.570 39265"
  },
  {
    "objectID": "date_times.html#creating-pre-post-variables",
    "href": "date_times.html#creating-pre-post-variables",
    "title": "3  Handling Times and Dates",
    "section": "3.3 Creating pre post variables",
    "text": "3.3 Creating pre post variables\nWhen doing financial linkage we tipically compute things like comp_revenue or comp_osat based on a comparison between two different periods of time. Thankfully, using lubridate, it is very easy to compute this type of variables, here we will go over four different approaches depending on how we are doing the splits:\n\nYear VS prior Year\n\n\n\nShow the code\nagged_by_year <- shel_csi%>%\n  mutate(period = case_when(year(VisitDate) == 2022 ~ \"prev\", year(VisitDate) == 2023 ~ \"current\"))%>%\n  group_by(period)%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n\n6 months VS 6 prior months\n\n\n\nShow the code\nagged_by_6months <- shel_csi%>%\n  mutate(period = case_when(month(VisitDate) %in% 1:6 ~ \"prev\", month(VisitDate) %in% 7:12 ~ \"current\"))%>%\n  group_by(period)%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n\n6 months VS same period previous year\n\n\n\nShow the code\nagged_by_6months_same <- shel_csi%>%\n  filter(month(VisitDate) %in% 1:6)%>%\n  mutate(period = case_when(year(VisitDate) == 2022 ~ \"prev\", year(VisitDate) == 2023 ~ \"current\"))%>%\n  group_by(period)%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n\nCustom split date\n\nIf we have a more specific date that we need to work around, lets say, we have a specific date where the fiscal year for this client begins, we can always do date comparisons to a more granular level. For example, if our client fiscal year starts on the 3rd of October each year and we are doing a YOY comparison between this fiscal year VS prior fiscal year, we can split our periods based on any date before “10/03/2022” as previous and any date equal or greater than that same date as our current period. Making sure of course that our original date range starts and end in the correct dates (beginning of prior fiscal year and end of current one)\nTo do that, lubridate lets us define dates using wrapper functions such as ymd(). To introduce this inside our case_when() call, we can either define the date as an object first and then call it inside the function, or we can simply call it inside it, here are the two approaches:\n\nTip: The ymd() function stands for year-month-date indicating the order and meaning of the numbers we provide inside it. As long as that is the order of the input provided the format can vary, so it would read “2023-10-03” or “2023/10/03” or “20231003” as the same date. However if we used ymd(\"2023/10/03\"), lubridate would interpret that as the 3rd of October of 2023, while if we did ydm(\"2023/10/03\"), lubridate would interpret that as the 10th of March instead. If you want to know all the possible wrappers for date definitions, I suggest you take a look at the lubridate documentation here:\n\n\n\nShow the code\nfiscal_year_start <- ymd(\"2023-10-03\")\nyoy_custom_date <- shel_csi%>%\n  mutate(period = case_when(VisitDate < fiscal_year_start ~ \"prev\", \n                            VisitDate >= fiscal_year_start ~ \"current\"))%>%\n  group_by(period)%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())\n\n\n\n\nShow the code\nyoy_custom_date <- shel_csi%>%\n  mutate(period = case_when(VisitDate < ymd(\"2023-10-03\") ~ \"prev\", \n                            VisitDate >= ymd(\"2023-10-03\") ~ \"current\"))%>%\n  group_by(period)%>%\n  summarise(mean_osat = mean(R003000 == 5, na.rm = T), count = n())"
  },
  {
    "objectID": "date_times.html#pre-post-analysis",
    "href": "date_times.html#pre-post-analysis",
    "title": "3  Handling Times and Dates",
    "section": "3.4 Pre post analysis",
    "text": "3.4 Pre post analysis\nFor a simple pre-post analysis, you can always replicate the steps shown above and simply use “pre” and “post” instead of “prev” and “current”. But there are other more complex scenarios where we may need some additional steps. For example, we may want to look at differences between renovations in stores for which we would have a different date for each store. For this example, we will use an old analysis for Shell Germany looking at differences before and after a cafe was opened in some gas stations. First we load the data:\n\n\nShow the code\n#get data\nconnection <- dbConnect(odbc(),\n                        Driver = \"SQL Server\",\n                        Server = \"SRV_INT_TRANS\\\\SMGTRANS\",\n                        Trusted_Connection = \"True\")\n\nshel_csi_deu <- dbGetQuery(connection,\n                             \"SELECT\nSHEL_CSI_Calls.[UID],\nSHEL_CSI_Calls.[StoreId],\nSHEL_CSI_Calls.[VisitDate],\nSHEL_CSI_Calls.[R003000],\nSHEL_CSI_Calls.[R011000], \nSHEL_CSI_Calls.[R026000] \nFROM\nedify.dbo.SHEL_CSI_Calls WITH (NOLOCK)  \nINNER JOIN structures.dbo.SHEL_CSI_Store_Info WITH (NOLOCK) ON SHEL_CSI_Calls.[StoreId] = SHEL_CSI_Store_Info.[StoreId]\nWHERE\nSHEL_CSI_Store_Info.[IN_CSI] = -1 AND SHEL_CSI_Store_Info.[TEST_STORE] = 0\nAND SHEL_CSI_Calls.[Date_Time] BETWEEN '2020-11-01 12:00:00 AM' AND '2023-01-31 11:59:59 PM'\nAND SHEL_CSI_Calls.[CountryID] = 'DEU'\")\n\n\n\nlibrary(readxl)\ncafe_list22 <- read_excel(\"S:/DBases/Shell/Analysis/2023/Market QBRs/DACH/Cafe pre-post analysis/Shell Cafe Site List - WEST - January 2023.xlsx\", \n                          sheet = \"DACH 2022\")\n\nstore_list21 <- read_excel(\"S:/DBases/Shell/Analysis/2023/Market QBRs/DACH/Cafe pre-post analysis/store_list21.xlsx\", \n                           sheet = \"Sheet2\", col_types = c(\"text\", \"date\"))"
  },
  {
    "objectID": "date_times.html#days-pre-post",
    "href": "date_times.html#days-pre-post",
    "title": "3  Handling Times and Dates",
    "section": "3.5 30 days pre-post",
    "text": "3.5 30 days pre-post\nOnce we have our data, we can go ahead and join it and create the period variable. First, lets try looking at differences in scores the 30 days before the cafe was opened and the 30 days after. Instead of calculating by hand what date that would be and then creating it as a custom date as we saw above, one really handy feature from lubridate is that it allows for arithmetic operations with date types. For example, we can select a specific date and then add or subtract time from it, from seconds to years. In this case we will take the LauncheDate variable and subtract 30 days for the pre and add 30 days for the post. Note that we are also perfomring some mutates before hand, first getting varaibles to a top box format and also making sure the date of launching is in a proper date format using the wrapper ymd(). For our case_when() defining the period variable, we will make anything else outside of our date range of interest be labeld as “OutRange”, we can then decide if we filter those out or keep them for reference.\n\n\nShow the code\npre_post_data <- shel_csi_deu%>%\n  inner_join(store_list21)%>%\n  mutate(OSAT = case_when(R003000 %in% 1:4 ~ 0, R003000 == 5 ~ 1), \n         NPS = case_when(R026000 %in% 0:6 ~ -1, R026000 %in% 7:8 ~ 0, R026000 %in% 9:10 ~ 1),\n         TLAG = case_when(R011000 %in% 1:4 ~ 0, R011000 == 5 ~ 1),\n         LaunchDate = ymd(LaunchDate))%>%\n  mutate(period = case_when(VisitDate < LaunchDate & VisitDate > LaunchDate - days(30) ~ \"pre\",\n                            VisitDate > LaunchDate & VisitDate < LaunchDate + days(30) ~ \"post\",\n                            TRUE ~ \"OutRange\"))\n\npre_post_data%>%\n  group_by(period)%>%\n  summarise(across(OSAT:TLAG, mean, na.rm = T), count = n())\n\n\n# A tibble: 3 x 5\n  period    OSAT   NPS  TLAG count\n  <chr>    <dbl> <dbl> <dbl> <int>\n1 OutRange 0.586 0.515 0.588  3565\n2 post     0.578 0.36  0.596   129\n3 pre      0.606 0.594 0.594   130"
  },
  {
    "objectID": "date_times.html#doing-a-pre-post-trend",
    "href": "date_times.html#doing-a-pre-post-trend",
    "title": "3  Handling Times and Dates",
    "section": "3.6 Doing a pre post trend",
    "text": "3.6 Doing a pre post trend\nBut what if we wanted to see how score change over time before and after the change? We could do a pre post trend as is shown below. Basically we are creating several groups based on how far the visit is from the openinning date. In this case we are doing 15, 30, 45, and 60 days before and after the cafe launch. As you can see, we simply extend our case_when() defining more specific ranges for each group\n\n\nShow the code\nshel_csi_deu%>%\n  inner_join(store_list21)%>%\n  mutate(OSAT = case_when(R003000 %in% 1:4 ~ 0, R003000 == 5 ~ 1), \n         NPS = case_when(R026000 %in% 0:6 ~ -1, R026000 %in% 7:8 ~ 0, R026000 %in% 9:10 ~ 1),\n         TLAG = case_when(R011000 %in% 1:4 ~ 0, R011000 == 5 ~ 1),\n         LaunchDate = ymd(LaunchDate))%>%\n  mutate(period = case_when(VisitDate < LaunchDate & VisitDate > LaunchDate - days(15) ~ \"pre15\",\n                            VisitDate < LaunchDate - days(15) & VisitDate > LaunchDate - days(30) ~ \"pre30\",\n                            VisitDate < LaunchDate - days(30) & VisitDate > LaunchDate - days(45) ~ \"pre45\",\n                            VisitDate < LaunchDate - days(45) & VisitDate > LaunchDate - days(60) ~ \"pre60\",\n                            VisitDate > LaunchDate & VisitDate < LaunchDate + days(15) ~ \"post15\",\n                            VisitDate > LaunchDate + days(15) & VisitDate < LaunchDate + days(30) ~ \"post30\",\n                            VisitDate > LaunchDate + days(30) & VisitDate < LaunchDate + days(45) ~ \"post45\",\n                            VisitDate > LaunchDate + days(45) & VisitDate < LaunchDate + days(60) ~ \"post60\",\n                            TRUE ~ \"OutRange\"))%>%\n  group_by(period)%>%\n  summarise(across(OSAT:TLAG, mean, na.rm = T), count = n())\n\n\n# A tibble: 9 x 5\n  period    OSAT   NPS  TLAG count\n  <chr>    <dbl> <dbl> <dbl> <int>\n1 OutRange 0.581 0.512 0.581  3318\n2 post15   0.508 0.316 0.6      72\n3 post30   0.674 0.419 0.591    57\n4 post45   0.545 0.333 0.558    56\n5 post60   0.725 0.622 0.711    49\n6 pre15    0.523 0.545 0.5      49\n7 pre30    0.662 0.629 0.661    81\n8 pre45    0.684 0.611 0.684    47\n9 pre60    0.695 0.642 0.744    95"
  }
]